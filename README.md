# üìò Explicaci√≥ dels Logs d'Apache amb Mistral-7B

Aquest repositori cont√© un script en Python que utilitza el model Mistral-7B de Hugging Face per analitzar i explicar els logs d'Apache, identificant errors i oferint informaci√≥ detallada.

---

## üóÇÔ∏è **Contingut del Repositori**
1. **[`script.py`](/home/adam/Escritorio/tests/mistral.py)**  
   Cont√© el codi Python per a consumir l'API de Mistral-7B i analitzar els logs d'Apache.
   
2. **`/venv`**  
   Entorn virtual necessari per executar el model, assegurant l'√∫s d'una versi√≥ espec√≠fica de Python sense afectar altres aplicacions.

---

## üîç **Definicions de Paraules Clau**
- **RAG (Retrieval-Augmented Generation):** M√®tode que permet recuperar informaci√≥ rellevant per millorar les respostes generades pel model.  
   *Exemple:* Si un log d'Apache mostra un error 404, el model pot recuperar informaci√≥ sobre les possibles causes i oferir una explicaci√≥ detallada.

- **FINE-TUNED:** Model que ha estat entrenat espec√≠ficament per a tasques com l'an√†lisi de logs i detecci√≥ d'errors concrets en servidors.

- **TEMPERATURE:** Par√†metre que controla l'aleatorietat de les respostes del model. Valors alts = m√©s creativitat; valors baixos = m√©s precisi√≥.  
   *Exemple:* Amb un valor de temperatura alt, el model podria suggerir m√∫ltiples explicacions per a un error; amb un valor baix, aniria directament a la causa m√©s probable.

---

## ‚öôÔ∏è **Funcionament**
1. Primer, cal consumir l'API de Hugging Face del model **Mistral-7B**, que pots trobar en el seg√ºent enlla√ß:  
   [üîó Mistral-7B Instruct v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3?inference_api=true&language=python&inference_provider=hf-inference)

2. Perqu√® l'API funcioni, √©s necessari crear un **token personal** al teu perfil de Hugging Face. El trobar√†s dins de la configuraci√≥ del teu compte.

3. Un cop generat el token i configurada l'API amb el model seleccionat, ja pots executar el script per analitzar els logs.

---

## ‚öñÔ∏è **Difer√®ncies entre Consumir l'API i Descarregar el Model**
| M√®tode          | Recursos Necessaris                                           | Avantatges                               |
|------------------|--------------------------------------------------------------|-----------------------------------------|
| **Descarregar** | - RAM requerida: **32 GB** (sense quantitzaci√≥)              | + Independ√®ncia de servidors externs    |
|                 | - GPU recomanada: **‚â•16 GB VRAM** (NVIDIA A100, RTX 3090...) | + Acc√©s immediat un cop carregat        |
|                 | - Amb quantitzaci√≥ (bitsandbytes 4-bit): m√≠nim **8-12 GB**   |                                         |
|                 | - Temps d'infer√®ncia: entre **10 segons i 2 minuts**         |                                         |
| **Consumir API** | - Nom√©s l'script Python                                     | + √ös m√≠nim de CPU i mem√≤ria             |
|                 | - Execuci√≥ en servidors de Hugging Face                      | + Sense necessitat de grans recursos    |

üîπ **Conclusi√≥:**  
Si no disposes d'una m√†quina amb altes prestacions, l'opci√≥ m√©s viable √©s consumir l'API. Aix√≤ et permet aprofitar el model sense saturar els recursos locals.

---

## üöÄ **Com Executar**
```bash
# Clonar el repositori
git clone https://github.com/llmopt/asix1-practica1-adamoughriss

# Accedir al directori
cd asix1-practica1-adamoughriss

# Activar l'entorn virtual
source venv/bin/activate

# Executar el script
python script.py
```

---

## ‚úçÔ∏è **Autor**  
Projecte desenvolupat per Adam Oughriss.

---
